{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "SpecialToken = r\"\"\"#(*+/\\:;<=[^_₩{|~‘“\"\"\" # 띄우기\n",
    "NotSpecialToken = r\"\"\",'\"]}>)”’\"\"\"   # 붙이기\n",
    "                            # 여기에 없으면 유지\n",
    "exclude = set(SpecialToken)\n",
    "unexclude = set(NotSpecialToken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특수문자 제거 함수\n",
    "def str2token(TokenList):\n",
    "    NewTokenList = []\n",
    "\n",
    "    for i in TokenList:\n",
    "        if i in exclude:\n",
    "            TokenList = TokenList.replace(i, ' ')\n",
    "\n",
    "        elif i in unexclude:\n",
    "            TokenList = TokenList.replace(i, '')\n",
    "\n",
    "    # NewTokenList = TokenList.split()\n",
    "    return TokenList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import json\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "from konlpy.tag import Komoran\n",
    "komoran = Komoran()\n",
    "\n",
    "os.chdir('../')\n",
    "import yake\n",
    "from yake.korea_token import edit_josa, edit_sentences\n",
    "os.chdir('test_dacon/')\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from collections import defaultdict\n",
    "\n",
    "from krwordrank.word import KRWordRank\n",
    "import math\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from pandas import DataFrame\n",
    "from test_testmain import RougeScorer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopword.txt 읽어오기 위한 path 및 파일이름\n",
    "language = 'ko'\n",
    "path = '../yake/StopwordsList/'\n",
    "txt = 'stopwords_{}.txt'.format(language)\n",
    "\n",
    "try:\n",
    "    f = open(path + txt, encoding='UTF-8')\n",
    "    ExistStopwords = set(f.read().split())\n",
    "    f.close()\n",
    "except FileNotFoundError:\n",
    "    ExistStopwords = [None]\n",
    "\n",
    "ExistStopwords = list(set(ExistStopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"valid_original.json\", \"r\", encoding=\"utf8\") as f:\n",
    "    contents = f.read() # string 타입\n",
    "    json_data = json.loads(contents)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kyake(txt:list):\n",
    "    # print(\"txt: \", txt)\n",
    "    kw_extractor = yake.KeywordExtractor(n=[1,2,3],top=50, stoplen=2, windowsSize=1, WG=False, ReDup=False)\n",
    "    return_tuple = kw_extractor.extract_keywords(''.join(txt))\n",
    "    # print(\"return_tuple: \", return_tuple)\n",
    "    return_list = [i[0] for i in return_tuple]\n",
    "    return_list = return_list[:10]\n",
    "    # print(\"return_list: \", return_list)\n",
    "\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_parser(txt:list):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectorizer.fit(txt)\n",
    "    matrix = vectorizer.transform(txt)\n",
    "\n",
    "    # vocabulary_word_id = defaultdict(int)\n",
    "    vocabulary_word_id = {token:idx for idx,token in enumerate(vectorizer.get_feature_names())}\n",
    "        \n",
    "    # 특징 추출 결과: {\"token\": value}\n",
    "    result = {token:matrix[0, vocabulary_word_id[token]] for token in vectorizer.get_feature_names()}\n",
    "        \n",
    "    # 내림차순 (중요도 high) 기준 정렬\n",
    "    result = sorted(result.items(), key = lambda item: item[1], reverse = True)\n",
    "\n",
    "    return_list = [i[0] for i in result if i[1] != 0 and i[0] not in ExistStopwords]\n",
    "    return_list = return_list[:10]\n",
    "\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_list = np.random.choice(range(0,30120),1000,replace=False)  # max: 30122 || 30122개의 뉴스 데이터 # random.randint(0, 30000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# * 키워드가 1-gram일 때 합성어, 고유명사만 출력 및 조사 떼는 실험"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deljosa(word:str):\n",
    "        '''\n",
    "        한국어 조사와 어말어미의 출현빈도 조사\n",
    "        상위 70개 통합 조사의 상대적 출현빈도\n",
    "        KT Data Set 99.8% 커버 조사\n",
    "        '''\n",
    "        if word[-4:] in ['으로부터',\"으로서의\"]:\n",
    "            return word[:-4]\n",
    "        elif word[-3:] in ['에서는','으로써','에서의','로부터','으로는',\n",
    "                        \"에서도\",\"까지의\",\"이라는\",\"으로의\",\"이라고\"\n",
    "                        ,\"보다는\",\"로서의\",\"만으로\"]:\n",
    "            return word[:-3]\n",
    "        elif word[-2:] in ['으로','에서','하고','이다','과를','보다','에는'\n",
    "                        ,'이나','로서','에게','까지','과는','만을',\"대로\"\n",
    "                        ,\"이고\",\"에도\",\"과의\",\"로의\",\"이며\",\"로써\",\"로는\"\n",
    "                        ,\"만이\",\"와의\",\"마다\",\"와는\",\"이라\",\"에만\",\"라고\"\n",
    "                        ,\"처럼\",\"부터\",\"로도\",'토록']:\n",
    "            return word[:-2]\n",
    "        elif word[-1:] in ['을',\"의\",\"를\",\"에\",\"은\",\"이\",\"는\",\"과\",\"로\",\"가\"\n",
    "                        ,\"와\",\"고\",\"여\",\"도\",\"다\",\"든\",\"라\",\"나\",\"만\",\"며\"\n",
    "                        ,\"께\",\"요\"]:\n",
    "            return word[:-1]\n",
    "        #print( word[:-3])\n",
    "        return word\n",
    "def strainer(txt):\n",
    "    return_number = []\n",
    "    return_value = []\n",
    "\n",
    "    for i in range(len(txt)):\n",
    "        if ' ' not in txt[i]:\n",
    "            if komoran.pos(txt[i])[0][1] in ['NNG','NR','NP']:\n",
    "                if len(komoran.pos(txt[i]))>1 and komoran.pos(txt[i])[1][1] in ['NNG','NNP','NNB','NR','NP']:\n",
    "                    # return_value.append(komoran.pos(txt[i])[0][0] + komoran.pos(txt[i])[1][0])\n",
    "                    return_number.append(i)\n",
    "            \n",
    "            elif komoran.pos(txt[i])[0][1] in ['NNP']:\n",
    "                return_number.append(i)\n",
    "\n",
    "            elif komoran.pos(txt[i])[0][1] in ['MM']:\n",
    "                if len(komoran.pos(txt[i]))>1 and komoran.pos(txt[i])[1][1] in ['NNG','NNP','NNB','NR','NP']:\n",
    "                    return_number.append(i)\n",
    "\n",
    "            elif komoran.pos(txt[i])[0][1] == 'XPN':\n",
    "                if len(komoran.pos(txt[i]))>1 and komoran.pos(txt[i])[1][1] in ['NNG','NNP','NR','NP']:\n",
    "                    return_number.append(i)\n",
    "                elif len(komoran.pos(txt[i]))>1 and komoran.pos(txt[i])[1][1] in ['VV', 'VA', 'VX', 'VC', 'XR']:\n",
    "                    if len(komoran.pos(txt[i]))>2 and komoran.pos(txt[i])[2][1] in ['ETN']:\n",
    "                        if len(komoran.pos(txt[i]))>3 and komoran.pos(txt[i])[3][1] in ['NNG']:\n",
    "                            return_number.append(i)\n",
    "\n",
    "            elif komoran.pos(txt[i])[0][1] == 'XR':\n",
    "                if len(komoran.pos(txt[i]))>1 and komoran.pos(txt[i])[1][1] in ['XSN', 'NNG','NR','NP']:\n",
    "                    return_number.append(i)\n",
    "\n",
    "            elif komoran.pos(txt[i])[0][1] == 'NA':\n",
    "                return_number.append(i)\n",
    "\n",
    "            elif komoran.pos(txt[i])[0][1] in ['SL', 'SH']:\n",
    "                if len(komoran.pos(txt[i]))>1 and komoran.pos(txt[i])[1][1] in ['NNG','NNP','NNB','NR','NP']:\n",
    "                    return_number.append(i)      \n",
    "                elif len(komoran.pos(txt[i]))>1 and komoran.pos(txt[i])[1][1] in ['VV', 'VA', 'VX', 'VC', 'XR']:\n",
    "                    if len(komoran.pos(txt[i]))>2 and komoran.pos(txt[i])[2][1] in ['ETN']:\n",
    "                        if len(komoran.pos(txt[i]))>3 and komoran.pos(txt[i])[3][1] in ['NNG']:\n",
    "                            return_number.append(i)\n",
    "\n",
    "            elif komoran.pos(txt[i])[0][1] in ['SN']:\n",
    "                if len(komoran.pos(txt[i]))>1 and komoran.pos(txt[i])[1][1] in ['NNG','NNP','NNB','NR','NP']:\n",
    "                    return_number.append(i)\n",
    "                elif len(komoran.pos(txt[i]))>1 and komoran.pos(txt[i])[1][1] in ['SW']:\n",
    "                    return_number.append(i)\n",
    "                elif len(komoran.pos(txt[i]))>2 and komoran.pos(txt[i])[1][1] in ['SF']:\n",
    "                    if len(komoran.pos(txt[i]))>3 and komoran.pos(txt[i])[2][1] in ['SN']:\n",
    "                        if len(komoran.pos(txt[i]))>4 and komoran.pos(txt[i])[3][1] in ['SW']:\n",
    "                            return_number.append(i)\n",
    "                        elif len(komoran.pos(txt[i]))>4 and komoran.pos(txt[i])[3][1] in ['NNG','NNP','NNB','NR','NP']:\n",
    "                            return_number.append(i)\n",
    "        else:\n",
    "            return_number.append(i)\n",
    "\n",
    "    # print('return_number: ', return_number)\n",
    "    for i in return_number:\n",
    "        return_value.append(txt[i])\n",
    "    # print(\"return_value: \", return_value)\n",
    "\n",
    "    return return_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kyake(txt:list):\n",
    "    # print(\"txt: \", txt)\n",
    "    kw_extractor = yake.KeywordExtractor(n=[1,2,3],top=50, stoplen=2, windowsSize=1, WG=False, ReDup=False)\n",
    "    return_tuple = kw_extractor.extract_keywords(''.join(txt))\n",
    "    # print(\"return_tuple: \", return_tuple)\n",
    "\n",
    "    return_list = [i[0] for i in return_tuple]\n",
    "    return_list = [deljosa(i) for i in strainer(return_list)]\n",
    "    return_list = [i for i in return_list if len(i) > 1]\n",
    "    \n",
    "    return_list = return_list[:10]\n",
    "    # print(\"return_list: \", return_list)\n",
    "\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEWS = np.random.choice(range(0,30120),30000,replace=False)  # max: 30122 || 30122개의 뉴스 데이터 # random.randint(0, 30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [18:22:22<00:00,  2.20s/it]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge_1의 평균점수:  0.474906216848075\n",
      "rouge_2의 평균점수:  0.351124266929438\n",
      "rouge_l의 평균점수:  0.43411827951858845\n",
      "rouge_w의 평균점수:  0.2715256369642409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_trainset = DataFrame(columns=[\"id\", \"summary\"])\n",
    "df_testset = DataFrame(columns=[\"id\", \"summary\"])\n",
    "\n",
    "scorer = RougeScorer()\n",
    "_dataframe = DataFrame()\n",
    "\n",
    "R1 = []\n",
    "R2 = []\n",
    "Rl = []\n",
    "Rw = []\n",
    "\n",
    "for i in tqdm(NEWS):\n",
    "    \n",
    "    id = None\n",
    "    topic_number = None\n",
    "    \n",
    "    text_list = [\n",
    "        str2token(json_data[\"documents\"][i][\"text\"][j][k][\"sentence\"] + '.') \n",
    "        for j in range(len(json_data[\"documents\"][i][\"text\"])) \n",
    "        for k in range(len(json_data[\"documents\"][i][\"text\"][j]))\n",
    "    ]\n",
    "\n",
    "    topic_number = json_data[\"documents\"][i][\"extractive\"]\n",
    "    id = [int(json_data[\"documents\"][0]['id'])]\n",
    "\n",
    "    text = [' '.join(text_list)]\n",
    "    STRTexT = ' '.join(text_list)\n",
    "    LISTText = text_list\n",
    "\n",
    "    new_text = edit_sentences(STRTexT)\n",
    "    total_value = [' '.join(edit_josa(new_text[x])) for x in range(len(new_text))]\n",
    "    TUNNEDText = total_value\n",
    "\n",
    "    ### Kyake 키워드 추출: kyake(text)                     || txt:list\n",
    "    keywords = kyake(text)\n",
    "\n",
    "    score = 0\n",
    "    for l in range(len(text_list)):         # text_list[l] : text_list의 l번째 문장\n",
    "        for m in range(len(keywords)):      # keywords : text파일의 문서를 각 함수들을 돌린 후 도출되는 키워드들.\n",
    "            if keywords[m] in text_list[l]:\n",
    "                score = score + 1\n",
    "\n",
    "        score_data = dict(zip([\"id\", \"index\", \"score\"], [i, l, score]))\n",
    "        score = 0\n",
    "        _dataframe = _dataframe.append(score_data, ignore_index=True)\n",
    "\n",
    "    multi_index = _dataframe.sort_values(by=['score', 'index'], ascending=[False, True]).groupby(\"id\").head(3)\n",
    "    multi_index = multi_index.sort_values(by=['id','score'], ascending=[True, False])\n",
    "\n",
    "    summary_number = [int (i) for i in list(multi_index[multi_index['id']==i]['index'])]\n",
    "\n",
    "    try:    \n",
    "        df_testset['id'] = id\n",
    "        df_testset['summary'] = [[text_list[int(summary_number[0])],text_list[int(summary_number[1])], text_list[int(summary_number[2])]]]\n",
    "        df_trainset['id'] = id\n",
    "        df_trainset['summary'] = [[text_list[int(topic_number[0])],text_list[int(topic_number[1])], text_list[int(topic_number[2])]]]\n",
    "    except TypeError:\n",
    "        print(\"TypeError 발생: \", TypeError)\n",
    "    \n",
    "    score_set = scorer.compute_rouge(df_trainset, df_testset)\n",
    "    rouge_1 = score_set[0]\n",
    "    rouge_2 = score_set[1]\n",
    "    rouge_l = score_set[2]\n",
    "    rouge_w = score_set[3]\n",
    "\n",
    "    R1.append(rouge_1)\n",
    "    R2.append(rouge_2)\n",
    "    Rl.append(rouge_l)\n",
    "    Rw.append(rouge_w)\n",
    "\n",
    "    # print(\"키워드들: \", keywords)\n",
    "    # print(i,\"번째 문단..\")\n",
    "    # print(\"선택 문장 : \", (summary_number))\n",
    "    # print([text_list[int(summary_number[0])],text_list[int(summary_number[1])], text_list[int(summary_number[2])]])\n",
    "    # print(\"정답지 : \", topic_number)\n",
    "    # print([text_list[int(topic_number[0])],text_list[int(topic_number[1])], text_list[int(topic_number[2])]])\n",
    "    # print()\n",
    "\n",
    "print(\"rouge_1의 평균점수: \", sum(R1) / len(R1))\n",
    "print(\"rouge_2의 평균점수: \", sum(R2) / len(R2))\n",
    "print(\"rouge_l의 평균점수: \", sum(Rl) / len(Rl))\n",
    "print(\"rouge_w의 평균점수: \", sum(Rw) / len(Rw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('.env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a0fc480af1f47fb618343e9d74e4d54da4e469394bb9bebde36f22e938ea4c4d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
