{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import json\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "from konlpy.tag import Komoran\n",
    "komoran = Komoran()\n",
    "\n",
    "os.chdir('../')\n",
    "import yake\n",
    "from yake.korea_token import edit_josa, edit_sentences\n",
    "os.chdir('test_dacon/')\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from collections import defaultdict\n",
    "\n",
    "from krwordrank.word import KRWordRank\n",
    "import math\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from pandas import DataFrame\n",
    "from test_testmain import RougeScorer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopword.txt 읽어오기 위한 path 및 파일이름\n",
    "language = 'ko'\n",
    "path = '../yake/StopwordsList/'\n",
    "txt = 'stopwords_{}.txt'.format(language)\n",
    "\n",
    "try:\n",
    "    f = open(path + txt, encoding='UTF-8')\n",
    "    ExistStopwords = set(f.read().split())\n",
    "    f.close()\n",
    "except FileNotFoundError:\n",
    "    ExistStopwords = [None]\n",
    "\n",
    "ExistStopwords = list(set(ExistStopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"valid_original.json\", \"r\", encoding=\"utf8\") as f:\n",
    "    contents = f.read() # string 타입\n",
    "    json_data = json.loads(contents)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noun_parser(txt:str):\n",
    "    noun = komoran.nouns(txt)\n",
    "    count = Counter(noun)\n",
    "\n",
    "    # 명사 빈도 카운트\n",
    "    noun_list = count.most_common(50)\n",
    "\n",
    "    return_list = [i[0] for i in noun_list if len(i[0]) > 1 and i[0] not in ExistStopwords]\n",
    "    return_list = return_list[:10]\n",
    "\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_parser(txt:list):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectorizer.fit(txt)\n",
    "    matrix = vectorizer.transform(txt)\n",
    "\n",
    "    # vocabulary_word_id = defaultdict(int)\n",
    "    vocabulary_word_id = {token:idx for idx,token in enumerate(vectorizer.get_feature_names())}\n",
    "        \n",
    "    # 특징 추출 결과: {\"token\": value}\n",
    "    result = {token:matrix[0, vocabulary_word_id[token]] for token in vectorizer.get_feature_names()}\n",
    "        \n",
    "    # 내림차순 (중요도 high) 기준 정렬\n",
    "    result = sorted(result.items(), key = lambda item: item[1], reverse = True)\n",
    "\n",
    "    return_list = [i[0] for i in result if i[1] != 0 and i[0] not in ExistStopwords]\n",
    "    return_list = return_list[:10]\n",
    "\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordrank_parser(txt:list):\n",
    "    wordrank_extractor = KRWordRank(min_count=5, max_length=10)\n",
    "    keywords, rank, graph = wordrank_extractor.extract(txt, num_keywords=100)\n",
    "\n",
    "    def make_vocab_score(keywords, scaling=None):\n",
    "        if scaling is None:\n",
    "            scaling = lambda x:math.sqrt(x)\n",
    "        return {word:scaling(rank) for word, rank in keywords.items()}\n",
    "\n",
    "    keywords = make_vocab_score(keywords)\n",
    "\n",
    "    return_list = [i for i in keywords if i not in ExistStopwords]\n",
    "    return_list = return_list[:10]\n",
    "    \n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens')\n",
    "\n",
    "def keybert_parser(txt:list, model):\n",
    "    \n",
    "    tokenized_doc = komoran.pos(txt)\n",
    "    tokenized_nouns = ' '.join([word[0] for word in tokenized_doc if word[1] in ['NNG','NNP']])\n",
    "\n",
    "    n_gram_range = (1, 3)\n",
    "    count = CountVectorizer(ngram_range=n_gram_range).fit([tokenized_nouns])\n",
    "    candidates = count.get_feature_names_out()\n",
    "\n",
    "    doc_embedding = model.encode([txt])\n",
    "    candidate_embeddings = model.encode(candidates)\n",
    "\n",
    "    top_n = 10\n",
    "    distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
    "    keywords = [candidates[index] for index in distances.argsort()[0][-top_n:]]\n",
    "\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kyake(txt:list):\n",
    "    # print(\"txt: \", txt)\n",
    "    kw_extractor = yake.KeywordExtractor(n=[1,2,3],top=30, stoplen=2, windowsSize=1, WG=False, ReDup=False)\n",
    "    return_tuple = kw_extractor.extract_keywords(''.join(txt))\n",
    "    # print(\"return_tuple: \", return_tuple)\n",
    "    return_list = [i[0] for i in return_tuple]\n",
    "    return_list = return_list[:10]\n",
    "    # print(\"return_list: \", return_list)\n",
    "\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [05:42<00:00,  2.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge_1의 평균점수:  0.4265763741125359\n",
      "rouge_2의 평균점수:  0.2914302183256145\n",
      "rouge_l의 평균점수:  0.3807695787165743\n",
      "rouge_w의 평균점수:  0.23431972013031416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "news_list = np.random.choice(range(0,30120),300,replace=False)  # max: 30122 || 30122개의 뉴스 데이터 # random.randint(0, 30000)\n",
    "df_trainset = DataFrame(columns=[\"id\", \"summary\"])\n",
    "df_testset = DataFrame(columns=[\"id\", \"summary\"])\n",
    "\n",
    "scorer = RougeScorer()\n",
    "_dataframe = DataFrame()\n",
    "\n",
    "R1 = []\n",
    "R2 = []\n",
    "Rl = []\n",
    "Rw = []\n",
    "\n",
    "for i in tqdm(news_list):\n",
    "    \n",
    "    id = None\n",
    "    text_list = []\n",
    "    topic_number = None\n",
    "\n",
    "    text_list = [\n",
    "        json_data[\"documents\"][i][\"text\"][j][k][\"sentence\"] + '.'\n",
    "        for j in range(len(json_data[\"documents\"][i][\"text\"]))\n",
    "        for k in range(len(json_data[\"documents\"][i][\"text\"][j]))\n",
    "    ]\n",
    "\n",
    "    topic_number = json_data[\"documents\"][i][\"extractive\"]\n",
    "    id = [int(json_data[\"documents\"][0]['id'])]\n",
    "\n",
    "    text = [' '.join(text_list)]\n",
    "    STRTexT = ' '.join(text_list)\n",
    "    LISTText = text_list\n",
    "\n",
    "    new_text = edit_sentences(STRTexT)\n",
    "    total_value = [' '.join(edit_josa(new_text[x])) for x in range(len(new_text))]\n",
    "    TUNNEDText = total_value\n",
    "\n",
    "    ### 빈도수 기반 명사 추출: noun_parser(STRTexT)      \t ||\ttxt:str\n",
    "    ### TF.IDF: tfidf_parser(TUNNEDText)    \t        ||\ttxt:list\n",
    "    ### KR-WordRank: wordrank_parser(TUNNEDText)\t    ||\ttxt:list\n",
    "    ### Korean KeyBERT\" keybert_parser(STRTexT, model) \t||\ttxt:list\n",
    "    ### Kyake 키워드 추출: kyake(text)                     || txt:list\n",
    "\n",
    "    # keywords = noun_parser(STRTexT)\n",
    "    # keywords = tfidf_parser(TUNNEDText)\n",
    "    # keywords = wordrank_parser(LISTText)  # TUNNEDText\n",
    "    # keywords = keybert_parser(STRTexT, model)\n",
    "    keywords = kyake(text)\n",
    "\n",
    "    score = 0\n",
    "    for l in range(len(text_list)):     # text_list[l] : text_list의 l번째 문장\n",
    "        for m in range(len(keywords)):  # keywords : text파일의 문서를 각 함수들을 돌린 후 도출되는 키워드들.\n",
    "            if keywords[m] in text_list[l]:\n",
    "                score = score + 1\n",
    "        score_data = dict(zip([\"id\", \"index\", \"score\"], [i, l, score]))\n",
    "        score = 0\n",
    "        _dataframe = _dataframe.append(score_data, ignore_index=True)\n",
    "\n",
    "    multi_index = _dataframe.sort_values(by=['score', 'index'], ascending=[False, True]).groupby(\"id\").head(3)\n",
    "    multi_index = multi_index.sort_values(by=['id','score'], ascending=[True, False])\n",
    "\n",
    "    summary_number = [int (i) for i in list(multi_index[multi_index['id']==i]['index'])]\n",
    "\n",
    "    df_testset['id'] = id\n",
    "    df_testset['summary'] = [[text_list[int(summary_number[0])],text_list[int(summary_number[1])], text_list[int(summary_number[2])]]]\n",
    "    df_trainset['id'] = id\n",
    "    df_trainset['summary'] = [[text_list[int(topic_number[0])],text_list[int(topic_number[1])], text_list[int(topic_number[2])]]]\n",
    "\n",
    "    score_set = scorer.compute_rouge(df_trainset, df_testset)\n",
    "    rouge_1 = score_set[0]\n",
    "    rouge_2 = score_set[1]\n",
    "    rouge_l = score_set[2]\n",
    "    rouge_w = score_set[3]\n",
    "\n",
    "    R1.append(rouge_1)\n",
    "    R2.append(rouge_2)\n",
    "    Rl.append(rouge_l)\n",
    "    Rw.append(rouge_w)\n",
    "\n",
    "    # print(\"키워드들: \", keywords)\n",
    "    # print(i,\"번째 문단..\")\n",
    "    # print(\"선택 문장 : \", (summary_number))\n",
    "    # print([text_list[int(summary_number[0])],text_list[int(summary_number[1])], text_list[int(summary_number[2])]])\n",
    "    # print(\"정답지 : \", topic_number)\n",
    "    # print([text_list[int(topic_number[0])],text_list[int(topic_number[1])], text_list[int(topic_number[2])]])\n",
    "    # print()\n",
    "\n",
    "print(\"rouge_1의 평균점수: \", sum(R1) / len(R1))\n",
    "print(\"rouge_2의 평균점수: \", sum(R2) / len(R2))\n",
    "print(\"rouge_l의 평균점수: \", sum(Rl) / len(Rl))\n",
    "print(\"rouge_w의 평균점수: \", sum(Rw) / len(Rw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 30000개 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_list = np.random.choice(range(0,30120),30000,replace=False)  # max: 30122 || 30122개의 뉴스 데이터 # random.randint(0, 30000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 13811/30000 [15:05:11<58:25,  4.62it/s]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TypeError 발생:  353136918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [16:29:14<00:00,  1.98s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge_1의 평균점수:  0.40289508598456664\n",
      "rouge_2의 평균점수:  0.25501628855825076\n",
      "rouge_l의 평균점수:  0.3500841162403251\n",
      "rouge_w의 평균점수:  0.2132214644427696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_trainset = DataFrame(columns=[\"id\", \"summary\"])\n",
    "df_testset = DataFrame(columns=[\"id\", \"summary\"])\n",
    "\n",
    "scorer = RougeScorer()\n",
    "_dataframe = DataFrame()\n",
    "\n",
    "R1 = []\n",
    "R2 = []\n",
    "Rl = []\n",
    "Rw = []\n",
    "\n",
    "for i in tqdm(news_list):\n",
    "    \n",
    "    id = None\n",
    "    topic_number = None\n",
    "    \n",
    "    text_list = [\n",
    "        json_data[\"documents\"][i][\"text\"][j][k][\"sentence\"] + '.' \n",
    "        for j in range(len(json_data[\"documents\"][i][\"text\"])) \n",
    "        for k in range(len(json_data[\"documents\"][i][\"text\"][j]))\n",
    "    ]\n",
    "\n",
    "    topic_number = json_data[\"documents\"][i][\"extractive\"]\n",
    "    id = [int(json_data[\"documents\"][0]['id'])]\n",
    "\n",
    "    text = [' '.join(text_list)]\n",
    "    STRTexT = ' '.join(text_list)\n",
    "    LISTText = text_list\n",
    "\n",
    "    new_text = edit_sentences(STRTexT)\n",
    "    total_value = [' '.join(edit_josa(new_text[x])) for x in range(len(new_text))]\n",
    "    TUNNEDText = total_value\n",
    "\n",
    "    ### 빈도수 기반 명사 추출: noun_parser(STRTexT)      \t ||\ttxt:str\n",
    "\n",
    "    \"\"\"\n",
    "    return : list\n",
    "    \"\"\"\n",
    "\n",
    "    keywords = noun_parser(STRTexT)\n",
    "\n",
    "    score = 0\n",
    "    for l in range(len(text_list)):         # text_list[l] : text_list의 l번째 문장\n",
    "        for m in range(len(keywords)):      # keywords : text파일의 문서를 각 함수들을 돌린 후 도출되는 키워드들.\n",
    "            if keywords[m] in text_list[l]:\n",
    "                score = score + 1\n",
    "        score_data = dict(zip([\"id\", \"index\", \"score\"], [i, l, score]))\n",
    "        score = 0\n",
    "        _dataframe = _dataframe.append(score_data, ignore_index=True)\n",
    "\n",
    "    multi_index = _dataframe.sort_values(by=['score', 'index'], ascending=[False, True]).groupby(\"id\").head(3)\n",
    "    multi_index = multi_index.sort_values(by=['id','score'], ascending=[True, False])\n",
    "\n",
    "    summary_number = [int (i) for i in list(multi_index[multi_index['id']==i]['index'])]\n",
    "\n",
    "    try:    \n",
    "        df_testset['id'] = id\n",
    "        df_testset['summary'] = [[text_list[int(summary_number[0])],text_list[int(summary_number[1])], text_list[int(summary_number[2])]]]\n",
    "        df_trainset['id'] = id\n",
    "        df_trainset['summary'] = [[text_list[int(topic_number[0])],text_list[int(topic_number[1])], text_list[int(topic_number[2])]]]\n",
    "    except TypeError:\n",
    "        print(\"TypeError 발생: \", json_data[\"documents\"][i][\"id\"])\n",
    "    \n",
    "    score_set = scorer.compute_rouge(df_trainset, df_testset)\n",
    "    rouge_1 = score_set[0]\n",
    "    rouge_2 = score_set[1]\n",
    "    rouge_l = score_set[2]\n",
    "    rouge_w = score_set[3]\n",
    "\n",
    "    R1.append(rouge_1)\n",
    "    R2.append(rouge_2)\n",
    "    Rl.append(rouge_l)\n",
    "    Rw.append(rouge_w)\n",
    "\n",
    "print(\"rouge_1의 평균점수: \", sum(R1) / len(R1))\n",
    "print(\"rouge_2의 평균점수: \", sum(R2) / len(R2))\n",
    "print(\"rouge_l의 평균점수: \", sum(Rl) / len(Rl))\n",
    "print(\"rouge_w의 평균점수: \", sum(Rw) / len(Rw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 13811/30000 [46:44<1:22:08,  3.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TypeError 발생:  353136918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [2:04:56<00:00,  4.00it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge_1의 평균점수:  0.49430827391830257\n",
      "rouge_2의 평균점수:  0.3755918322511678\n",
      "rouge_l의 평균점수:  0.4550028104580825\n",
      "rouge_w의 평균점수:  0.28702157616674867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_trainset = DataFrame(columns=[\"id\", \"summary\"])\n",
    "df_testset = DataFrame(columns=[\"id\", \"summary\"])\n",
    "\n",
    "scorer = RougeScorer()\n",
    "_dataframe = DataFrame()\n",
    "\n",
    "R1 = []\n",
    "R2 = []\n",
    "Rl = []\n",
    "Rw = []\n",
    "\n",
    "for i in tqdm(news_list):\n",
    "    \n",
    "    id = None\n",
    "    topic_number = None\n",
    "    \n",
    "    text_list = [\n",
    "        json_data[\"documents\"][i][\"text\"][j][k][\"sentence\"] + '.' \n",
    "        for j in range(len(json_data[\"documents\"][i][\"text\"])) \n",
    "        for k in range(len(json_data[\"documents\"][i][\"text\"][j]))\n",
    "    ]\n",
    "\n",
    "    topic_number = json_data[\"documents\"][i][\"extractive\"]\n",
    "    id = [int(json_data[\"documents\"][0]['id'])]\n",
    "\n",
    "    text = [' '.join(text_list)]\n",
    "    STRTexT = ' '.join(text_list)\n",
    "    LISTText = text_list\n",
    "\n",
    "    new_text = edit_sentences(STRTexT)\n",
    "    total_value = [' '.join(edit_josa(new_text[x])) for x in range(len(new_text))]\n",
    "    TUNNEDText = total_value\n",
    "\n",
    "    ### TF.IDF: tfidf_parser(TUNNEDText)    \t        ||\ttxt:list\n",
    "\n",
    "    \"\"\"\n",
    "    return : list\n",
    "    \"\"\"\n",
    "\n",
    "    keywords = tfidf_parser(LISTText)\n",
    "\n",
    "    score = 0\n",
    "    for l in range(len(LISTText)):         # text_list[l] : text_list의 l번째 문장\n",
    "        for m in range(len(keywords)):     # keywords : text파일의 문서를 각 함수들을 돌린 후 도출되는 키워드들.\n",
    "            if keywords[m] in LISTText[l]:\n",
    "                score = score + 1\n",
    "        score_data = dict(zip([\"id\", \"index\", \"score\"], [i, l, score]))\n",
    "        score = 0\n",
    "        _dataframe = _dataframe.append(score_data, ignore_index=True)\n",
    "\n",
    "    multi_index = _dataframe.sort_values(by=['score', 'index'], ascending=[False, True]).groupby(\"id\").head(3)\n",
    "    multi_index = multi_index.sort_values(by=['id','score'], ascending=[True, False])\n",
    "\n",
    "    summary_number = [int (i) for i in list(multi_index[multi_index['id']==i]['index'])]\n",
    "\n",
    "    try:    \n",
    "        df_testset['id'] = id\n",
    "        df_testset['summary'] = [[text_list[int(summary_number[0])],text_list[int(summary_number[1])], text_list[int(summary_number[2])]]]\n",
    "        df_trainset['id'] = id\n",
    "        df_trainset['summary'] = [[text_list[int(topic_number[0])],text_list[int(topic_number[1])], text_list[int(topic_number[2])]]]\n",
    "    except TypeError:\n",
    "        print(\"TypeError 발생: \", json_data[\"documents\"][i][\"id\"])\n",
    "    \n",
    "    score_set = scorer.compute_rouge(df_trainset, df_testset)\n",
    "    rouge_1 = score_set[0]\n",
    "    rouge_2 = score_set[1]\n",
    "    rouge_l = score_set[2]\n",
    "    rouge_w = score_set[3]\n",
    "\n",
    "    R1.append(rouge_1)\n",
    "    R2.append(rouge_2)\n",
    "    Rl.append(rouge_l)\n",
    "    Rw.append(rouge_w)\n",
    "    \n",
    "print(\"rouge_1의 평균점수: \", sum(R1) / len(R1))\n",
    "print(\"rouge_2의 평균점수: \", sum(R2) / len(R2))\n",
    "print(\"rouge_l의 평균점수: \", sum(Rl) / len(Rl))\n",
    "print(\"rouge_w의 평균점수: \", sum(Rw) / len(Rw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* wordrank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 13811/30000 [1:44:53<50:05,  5.39it/s]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TypeError 발생:  353136918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [3:00:12<00:00,  2.77it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge_1의 평균점수:  0.4164455377369773\n",
      "rouge_2의 평균점수:  0.2728152794166633\n",
      "rouge_l의 평균점수:  0.3662498021335229\n",
      "rouge_w의 평균점수:  0.22441687718580353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_trainset = DataFrame(columns=[\"id\", \"summary\"])\n",
    "df_testset = DataFrame(columns=[\"id\", \"summary\"])\n",
    "\n",
    "scorer = RougeScorer()\n",
    "_dataframe = DataFrame()\n",
    "\n",
    "R1 = []\n",
    "R2 = []\n",
    "Rl = []\n",
    "Rw = []\n",
    "\n",
    "for i in tqdm(news_list):\n",
    "    \n",
    "    id = None\n",
    "    topic_number = None\n",
    "    \n",
    "    text_list = [\n",
    "        json_data[\"documents\"][i][\"text\"][j][k][\"sentence\"] + '.' \n",
    "        for j in range(len(json_data[\"documents\"][i][\"text\"])) \n",
    "        for k in range(len(json_data[\"documents\"][i][\"text\"][j]))\n",
    "    ]\n",
    "\n",
    "    topic_number = json_data[\"documents\"][i][\"extractive\"]\n",
    "    id = [int(json_data[\"documents\"][0]['id'])]\n",
    "\n",
    "    text = [' '.join(text_list)]\n",
    "    STRTexT = ' '.join(text_list)\n",
    "    LISTText = text_list\n",
    "\n",
    "    new_text = edit_sentences(STRTexT)\n",
    "    total_value = [' '.join(edit_josa(new_text[x])) for x in range(len(new_text))]\n",
    "    TUNNEDText = total_value\n",
    "\n",
    "    ### KR-WordRank: wordrank_parser(TUNNEDText)\t    ||\ttxt:list\n",
    "\n",
    "    \"\"\"\n",
    "    return : list\n",
    "    \"\"\"\n",
    "\n",
    "    keywords = wordrank_parser(LISTText)  # TUNNEDText\n",
    "\n",
    "    score = 0\n",
    "    for l in range(len(text_list)):         # text_list[l] : text_list의 l번째 문장\n",
    "        for m in range(len(keywords)):      # keywords : text파일의 문서를 각 함수들을 돌린 후 도출되는 키워드들.\n",
    "            if keywords[m] in text_list[l]:\n",
    "                score = score + 1\n",
    "        score_data = dict(zip([\"id\", \"index\", \"score\"], [i, l, score]))\n",
    "        score = 0\n",
    "        _dataframe = _dataframe.append(score_data, ignore_index=True)\n",
    "\n",
    "    multi_index = _dataframe.sort_values(by=['score', 'index'], ascending=[False, True]).groupby(\"id\").head(3)\n",
    "    multi_index = multi_index.sort_values(by=['id','score'], ascending=[True, False])\n",
    "\n",
    "    summary_number = [int (i) for i in list(multi_index[multi_index['id']==i]['index'])]\n",
    "\n",
    "    try:    \n",
    "        df_testset['id'] = id\n",
    "        df_testset['summary'] = [[text_list[int(summary_number[0])],text_list[int(summary_number[1])], text_list[int(summary_number[2])]]]\n",
    "        df_trainset['id'] = id\n",
    "        df_trainset['summary'] = [[text_list[int(topic_number[0])],text_list[int(topic_number[1])], text_list[int(topic_number[2])]]]\n",
    "    except TypeError:\n",
    "        print(\"TypeError 발생: \", json_data[\"documents\"][i][\"id\"])\n",
    "    \n",
    "    score_set = scorer.compute_rouge(df_trainset, df_testset)\n",
    "    rouge_1 = score_set[0]\n",
    "    rouge_2 = score_set[1]\n",
    "    rouge_l = score_set[2]\n",
    "    rouge_w = score_set[3]\n",
    "\n",
    "    R1.append(rouge_1)\n",
    "    R2.append(rouge_2)\n",
    "    Rl.append(rouge_l)\n",
    "    Rw.append(rouge_w)\n",
    "\n",
    "print(\"rouge_1의 평균점수: \", sum(R1) / len(R1))\n",
    "print(\"rouge_2의 평균점수: \", sum(R2) / len(R2))\n",
    "print(\"rouge_l의 평균점수: \", sum(Rl) / len(Rl))\n",
    "print(\"rouge_w의 평균점수: \", sum(Rw) / len(Rw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Kyake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "SpecialToken = r\"\"\"#(*+/\\:;<=[^_₩{|~‘“\"\"\" # 띄우기\n",
    "NotSpecialToken = r\"\"\",'\"]}>)”’\"\"\"   # 붙이기\n",
    "                            # 여기에 없으면 유지\n",
    "exclude = set(SpecialToken)\n",
    "unexclude = set(NotSpecialToken)\n",
    "\n",
    "# 특수문자 제거 함수\n",
    "def str2token(TokenList):\n",
    "    NewTokenList = []\n",
    "\n",
    "    for i in TokenList:\n",
    "        if i in exclude:\n",
    "            TokenList = TokenList.replace(i, ' ')\n",
    "\n",
    "        elif i in unexclude:\n",
    "            TokenList = TokenList.replace(i, '')\n",
    "\n",
    "    # NewTokenList = TokenList.split()\n",
    "    return TokenList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 13811/30000 [1:24:31<1:48:09,  2.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TypeError 발생:  353136918\n",
      "TypeError 발생:  <class 'TypeError'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [17:51:20<00:00,  2.14s/it]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge_1의 평균점수:  0.4866184531288278\n",
      "rouge_2의 평균점수:  0.36551729143361406\n",
      "rouge_l의 평균점수:  0.4469727954869353\n",
      "rouge_w의 평균점수:  0.28046706763847146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_trainset = DataFrame(columns=[\"id\", \"summary\"])\n",
    "df_testset = DataFrame(columns=[\"id\", \"summary\"])\n",
    "\n",
    "scorer = RougeScorer()\n",
    "_dataframe = DataFrame()\n",
    "\n",
    "R1 = []\n",
    "R2 = []\n",
    "Rl = []\n",
    "Rw = []\n",
    "\n",
    "for i in tqdm(news_list):\n",
    "    \n",
    "    id = None\n",
    "    topic_number = None\n",
    "    \n",
    "    text_list = [\n",
    "        str2token(json_data[\"documents\"][i][\"text\"][j][k][\"sentence\"] + '.') \n",
    "        for j in range(len(json_data[\"documents\"][i][\"text\"])) \n",
    "        for k in range(len(json_data[\"documents\"][i][\"text\"][j]))\n",
    "    ]\n",
    "\n",
    "    topic_number = json_data[\"documents\"][i][\"extractive\"]\n",
    "    id = [int(json_data[\"documents\"][0]['id'])]\n",
    "\n",
    "    text = [' '.join(text_list)]\n",
    "    STRTexT = ' '.join(text_list)\n",
    "    LISTText = text_list\n",
    "\n",
    "    new_text = edit_sentences(STRTexT)\n",
    "    total_value = [' '.join(edit_josa(new_text[x])) for x in range(len(new_text))]\n",
    "    TUNNEDText = total_value\n",
    "\n",
    "    ### Kyake 키워드 추출: kyake(text)                     || txt:list\n",
    "\n",
    "    \"\"\"\n",
    "    return : list\n",
    "    \"\"\"\n",
    "\n",
    "    keywords = kyake(text)\n",
    "\n",
    "    score = 0\n",
    "    for l in range(len(text_list)):         # text_list[l] : text_list의 l번째 문장\n",
    "        for m in range(len(keywords)):      # keywords : text파일의 문서를 각 함수들을 돌린 후 도출되는 키워드들.\n",
    "            if keywords[m] in text_list[l]:\n",
    "                score = score + 1\n",
    "        score_data = dict(zip([\"id\", \"index\", \"score\"], [i, l, score]))\n",
    "        score = 0\n",
    "        _dataframe = _dataframe.append(score_data, ignore_index=True)\n",
    "\n",
    "    multi_index = _dataframe.sort_values(by=['score', 'index'], ascending=[False, True]).groupby(\"id\").head(3)\n",
    "    multi_index = multi_index.sort_values(by=['id','score'], ascending=[True, False])\n",
    "\n",
    "    summary_number = [int (i) for i in list(multi_index[multi_index['id']==i]['index'])]\n",
    "\n",
    "    try:    \n",
    "        df_testset['id'] = id\n",
    "        df_testset['summary'] = [[text_list[int(summary_number[0])],text_list[int(summary_number[1])], text_list[int(summary_number[2])]]]\n",
    "        df_trainset['id'] = id\n",
    "        df_trainset['summary'] = [[text_list[int(topic_number[0])],text_list[int(topic_number[1])], text_list[int(topic_number[2])]]]\n",
    "    except TypeError:\n",
    "        print(\"TypeError 발생: \", json_data[\"documents\"][i][\"id\"])\n",
    "    \n",
    "    score_set = scorer.compute_rouge(df_trainset, df_testset)\n",
    "    rouge_1 = score_set[0]\n",
    "    rouge_2 = score_set[1]\n",
    "    rouge_l = score_set[2]\n",
    "    rouge_w = score_set[3]\n",
    "\n",
    "    R1.append(rouge_1)\n",
    "    R2.append(rouge_2)\n",
    "    Rl.append(rouge_l)\n",
    "    Rw.append(rouge_w)\n",
    "\n",
    "print(\"rouge_1의 평균점수: \", sum(R1) / len(R1))\n",
    "print(\"rouge_2의 평균점수: \", sum(R2) / len(R2))\n",
    "print(\"rouge_l의 평균점수: \", sum(Rl) / len(Rl))\n",
    "print(\"rouge_w의 평균점수: \", sum(Rw) / len(Rw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* KeyBERT 모델은 기본 점수가 좋지 않을 뿐더러 시간이 오래 걸리는 관계로 생략함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('.env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a0fc480af1f47fb618343e9d74e4d54da4e469394bb9bebde36f22e938ea4c4d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
