{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import random\n",
    "# from collections import Counter\n",
    "\n",
    "# from konlpy.tag import Komoran\n",
    "# komoran = Komoran()\n",
    "\n",
    "# from yake import korea_token\n",
    "# from yake.korea_token import edit_josa, edit_sentences\n",
    "\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "# from collections import defaultdict\n",
    "\n",
    "# from krwordrank.word import KRWordRank\n",
    "# import math\n",
    "\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# from pandas import DataFrame\n",
    "# from test_testmain import RougeScorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요 라이브러리 선언\n",
    "from pprint import pprint\n",
    "\n",
    "from yake import korea_token\n",
    "from yake.korea_token import edit_josa, edit_sentences\n",
    "import math\n",
    "\n",
    "from konlpy.tag import Komoran\n",
    "from collections import Counter\n",
    "\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "from math import log\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import urllib.request\n",
    "np.random.seed(seed=0)\n",
    "\n",
    "from krwordrank.word import KRWordRank\n",
    "from krwordrank.sentence import summarize_with_sentences\n",
    "from krwordrank.word import summarize_with_keywords\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "from soynlp.tokenizer import MaxScoreTokenizer\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "import networkx\n",
    "\n",
    "# from textrank import KeywordSummarizer\n",
    "\n",
    "import itertools\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.ko.stop_words import STOP_WORDS\n",
    "from string import punctuation\n",
    "from heapq import nlargest\n",
    "\n",
    "import random\n",
    "from yake import yake\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import json\n",
    "from test_testmain import RougeScorer\n",
    "from yake.korea_token import edit_sentences\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"valid_original.json\", \"r\", encoding=\"utf8\") as f:\n",
    "    contents = f.read() # string 타입\n",
    "    json_data = json.loads(contents)\n",
    "    f.close()\n",
    "\n",
    "news_list = []\n",
    "for _ in range(1000):\n",
    "    news_list.append(random.randint(0, 30000))  # max: 30122 || 30122개의 뉴스 데이터\n",
    "\n",
    "news_set = set(news_list)\n",
    "news_list = list(news_set)\n",
    "\n",
    "# for i in range(len(news_list)):\n",
    "i=0\n",
    "id = None\n",
    "text_list = []\n",
    "topic_number = None\n",
    "\n",
    "for j in range(0,len(json_data[\"documents\"][i][\"text\"]),1):\n",
    "    for f in range(0,len(json_data[\"documents\"][i][\"text\"][j]),1):\n",
    "        # print(json_data[\"documents\"][i][\"text\"][j][f][\"sentence\"])\n",
    "        elements = json_data[\"documents\"][i][\"text\"][j][f][\"sentence\"] + '.'\n",
    "        text_list.append(elements)\n",
    "\n",
    "topic_number = json_data[\"documents\"][i][\"extractive\"]\n",
    "id = [int(json_data[\"documents\"][0]['id'])]\n",
    "\n",
    "text = [' '.join(text_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STRTexT = ' '.join(text_list)\n",
    "LISTText = text_list\n",
    "\n",
    "new_text = edit_sentences(STRTexT)\n",
    "# print('원본 split_sentences :', new_text)\n",
    "total_value = []\n",
    "for x in range(len(new_text)):\n",
    "    total_value.append(' '.join(edit_josa(new_text[x])))\n",
    "TUNNEDText = total_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noun_parser(txt:str):\n",
    "    noun = komoran.nouns(txt)\n",
    "    count = Counter(noun)\n",
    "\n",
    "    # 명사 빈도 카운트q\n",
    "    noun_list = count.most_common(10)\n",
    "    return_list = []\n",
    "    for i in noun_list:\n",
    "        return_list.append(i[0])\n",
    "\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_parser(txt:list):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectorizer.fit(txt)\n",
    "    matrix = vectorizer.transform(txt)\n",
    "\n",
    "    vocabulary_word_id = defaultdict(int)\n",
    "        \n",
    "    for idx, token in enumerate(vectorizer.get_feature_names()):\n",
    "        vocabulary_word_id[token] = idx\n",
    "        \n",
    "    # 특징 추출 결과: {\"token\": value}\n",
    "    result = defaultdict(str)\n",
    "        \n",
    "    for token in vectorizer.get_feature_names():\n",
    "        result[token] = matrix[0, vocabulary_word_id[token]]\n",
    "        \n",
    "    # 내림차순 (중요도 high) 기준 정렬\n",
    "    result = sorted(result.items(), key = lambda item: item[1], reverse = True)\n",
    "    return_list = []\n",
    "    for i in result:\n",
    "        if i[1] == 0:\n",
    "            continue\n",
    "        else:\n",
    "            if len(return_list) < 10:\n",
    "                return_list.append(i[0])\n",
    "            else:\n",
    "                continue\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordrank_parser(txt:list):\n",
    "    wordrank_extractor = KRWordRank(min_count=5, max_length=10)\n",
    "    keywords, rank, graph = wordrank_extractor.extract(txt, num_keywords=100)\n",
    "\n",
    "    def make_vocab_score(keywords, scaling=None):\n",
    "        if scaling is None:\n",
    "            scaling = lambda x:math.sqrt(x)\n",
    "        return {word:scaling(rank) for word, rank in keywords.items()}\n",
    "\n",
    "    keywords = make_vocab_score(keywords)\n",
    "    return_list = []\n",
    "    for i in keywords:\n",
    "        if len(return_list) < 10:\n",
    "            return_list.append(i[0])\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens')\n",
    "\n",
    "def keybert_parser(txt:list, model):\n",
    "    \n",
    "    tokenized_doc = komoran.pos(txt)\n",
    "    tokenized_nouns = ' '.join([word[0] for word in tokenized_doc if word[1] in ['NNG','NNP']])\n",
    "\n",
    "    # print('품사 태깅 10개만 출력 :',tokenized_doc[:10])\n",
    "    # print('명사 추출 :',tokenized_nouns)\n",
    "\n",
    "    n_gram_range = (1, 3)\n",
    "    count = CountVectorizer(ngram_range=n_gram_range).fit([tokenized_nouns])\n",
    "    candidates = count.get_feature_names_out()\n",
    "\n",
    "    # print('trigram 개수: ', len(candidates))\n",
    "    # print('trigram 5개 출력: ', candidates[:5])\n",
    "\n",
    "\n",
    "    doc_embedding = model.encode([txt])\n",
    "    candidate_embeddings = model.encode(candidates)\n",
    "\n",
    "    top_n = 10\n",
    "    distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
    "    keywords = [candidates[index] for index in distances.argsort()[0][-top_n:]]\n",
    "\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kw_extractor = yake.KeywordExtractor(n=[1,2,3],top=20, stoplen=2, windowsSize=1, WG=False, ReDup=False)\n",
    "\n",
    "df_trainset = DataFrame(columns=[\"id\", \"summary\"])\n",
    "df_testset = DataFrame(columns=[\"id\", \"summary\"])\n",
    "\n",
    "scorer = RougeScorer()\n",
    "_dataframe = DataFrame()\n",
    "\n",
    "R1 = []\n",
    "R2 = []\n",
    "Rl = []\n",
    "\n",
    "for i in range(len(news_list)):\n",
    "    \n",
    "    id = None\n",
    "    text_list = []\n",
    "    topic_number = None\n",
    "\n",
    "\n",
    "    for j in range(0,len(json_data[\"documents\"][i][\"text\"]),1):\n",
    "        for f in range(0,len(json_data[\"documents\"][i][\"text\"][j]),1):\n",
    "            # print(json_data[\"documents\"][i][\"text\"][j][f][\"sentence\"])\n",
    "            elements = json_data[\"documents\"][i][\"text\"][j][f][\"sentence\"] + '.'\n",
    "            text_list.append(elements)\n",
    "\n",
    "    topic_number = json_data[\"documents\"][i][\"extractive\"]\n",
    "    id = [int(json_data[\"documents\"][0]['id'])]\n",
    "\n",
    "    text = [' '.join(text_list)]\n",
    "\n",
    "\n",
    "    STRTexT = ' '.join(text_list)\n",
    "    LISTText = text_list\n",
    "\n",
    "    new_text = edit_sentences(STRTexT)\n",
    "    # print('원본 split_sentences :', new_text)\n",
    "    total_value = []\n",
    "    for x in range(len(new_text)):\n",
    "        total_value.append(' '.join(edit_josa(new_text[x])))\n",
    "    TUNNEDText = total_value\n",
    "\n",
    "\n",
    "    ### 빈도수 기반 명사 추출: noun_parser(STRTexT)      \t ||\ttxt:str\n",
    "\n",
    "    ### TF.IDF: tfidf_parser(TUNNEDText)    \t        ||\ttxt:list\n",
    "\n",
    "    ### KR-WordRank: wordrank_parser(TUNNEDText)\t    ||\ttxt:list\n",
    "\n",
    "    ### Korean KeyBERT\" keybert_parser(STRTexT, model) \t||\ttxt:list\n",
    "\n",
    "    ### return : list\n",
    "\n",
    "    # for i in range(len(text_list)):                        # range(len(text_list)) : n, i : [0,1,...,n]\n",
    "    # keywords = kw_extractor.extract_keywords(''.join(text))  # keywords : text[i]의 키워드 및 점수로 이루어진 튜플 리스트\n",
    "    \n",
    "    keywords = noun_parser(STRTexT)\n",
    "    # keywords = tfidf_parser(TUNNEDText)\n",
    "    # keywords = wordrank_parser(TUNNEDText)\n",
    "    # keywords = keybert_parser(STRTexT, model)\n",
    "    score = 0\n",
    "    for m in range(len(text_list)):     # text_list[m] : text_list의 m번째 문장\n",
    "        for l in keywords:              # keywords : text파일의 문서를 각 함수들을 돌린 후 도출되는 키워드들.\n",
    "            if l in text_list[m]:\n",
    "                score = score + 1*(1+0.2*len(l[0].split()))\n",
    "        score_data = dict(zip([\"id\", \"index\", \"score\"], [i, m, score]))\n",
    "        score = 0\n",
    "        _dataframe = _dataframe.append(score_data, ignore_index=True)\n",
    "\n",
    "\n",
    "    multi_index = _dataframe.sort_values(by=['score', 'index'], ascending=[False, True]).groupby(\"id\").head(3)\n",
    "    # print(multi_index)\n",
    "    multi_index = multi_index.sort_values(by=['id','score'], ascending=[True, False])\n",
    "    # print(multi_index)\n",
    "    # print(multi_index[multi_index['id']==i])\n",
    "\n",
    "    summary_number = list(multi_index[multi_index['id']==i]['index'])\n",
    "\n",
    "    df_testset['id'] = id\n",
    "    df_testset['summary'] = [[text_list[int(summary_number[0])],text_list[int(summary_number[1])], text_list[int(summary_number[2])]]]\n",
    "    df_trainset['id'] = id\n",
    "    df_trainset['summary'] = [[text_list[int(topic_number[0])],text_list[int(topic_number[1])], text_list[int(topic_number[2])]]]\n",
    "\n",
    "    score_set = scorer.compute_rouge(df_trainset, df_testset)\n",
    "    rouge_1 = score_set[0]\n",
    "    rouge_2 = score_set[1]\n",
    "    rouge_l = score_set[2]\n",
    "\n",
    "    R1.append(rouge_1)\n",
    "    R2.append(rouge_2)\n",
    "    Rl.append(rouge_l)\n",
    "\n",
    "    print(i,\"번째 문단..\")\n",
    "    print(\"내가 선택한 문장은..? : \", (summary_number))\n",
    "    print([text_list[int(summary_number[0])],text_list[int(summary_number[1])], text_list[int(summary_number[2])]])\n",
    "    print(\"정답지는..! : \", topic_number)\n",
    "    print([text_list[int(topic_number[0])],text_list[int(topic_number[1])], text_list[int(topic_number[2])]])\n",
    "    print()\n",
    "\n",
    "print(\"rouge_1의 평균점수: \", sum(R1) / len(R1))\n",
    "print(\"rouge_2의 평균점수: \", sum(R2) / len(R2))\n",
    "print(\"rouge_l의 평균점수: \", sum(Rl) / len(Rl))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "60e1a874c6243e5bf586dccf1bcb0e1eb4a1159cd049fa7b411ccd0064ced1f3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
